{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Index Data Pipeline Framework\n",
    "Small scale testing of general index data pipeline & graph NLP visualization using a test dataset of ~750 manuscripts\n",
    "across 2 topics: Antediluvian and Hennig86. \n",
    "\n",
    "---\n",
    "Created 6/3/22 by Ian Hay   \n",
    "Updated 7/23/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Dependencies\n",
    "\n",
    "---\n",
    "[Python 3.8+](https://www.python.org/downloads/release/python-380/)    \n",
    "[Pandas](https://pandas.pydata.org/)   \n",
    "[NumPy](https://numpy.org/)    \n",
    "[NetworkX](https://networkx.org/)  \n",
    "[pyvis](https://pyvis.readthedocs.io/en/latest/install.html)   \n",
    "[scikit-learn](https://scikit-learn.org/stable/index.html)  \n",
    "[seaborn](https://seaborn.pydata.org/)  \n",
    "[nltk](https://www.nltk.org/)   \n",
    "[gensim](https://pypi.org/project/gensim/)  \n",
    "[spacy](https://pypi.org/project/spacy/)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ian/opt/anaconda3/envs/generalindexenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "import ssl\n",
    "import gensim\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from pyvis import network as net\n",
    "from pyvis.network import Network\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing -------------------------------------------------------------------------------------------\n",
    "\n",
    "def getDocColumn(df, column, newColumnName):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a column of datatype list, constructs\n",
    "    a new column newColumnName with the list joined into a single\n",
    "    string and items separated by spaces (\" \").\n",
    "    \"\"\"\n",
    "    strDict = {}\n",
    "    for row in range(len(df)):\n",
    "        text = \". \".join(df.iloc[row][column])\n",
    "        text = text + \".\"\n",
    "        strDict[df.index[row]] = text\n",
    "    dfStr = pd.Series(strDict, name=newColumnName)\n",
    "    df[newColumnName] = dfStr\n",
    "\n",
    "def getBagOfWordsDF(df, docColumn):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    countvec = CountVectorizer()\n",
    "    X = countvec.fit_transform(df[docColumn])\n",
    "    dfBOW = pd.DataFrame(X.toarray(),columns=countvec.get_feature_names_out())\n",
    "    dfBOW.set_index(df.index, inplace=True)\n",
    "    return dfBOW\n",
    "\n",
    "def getNounsAndVerbs(df, column, newColumnName):\n",
    "    \"\"\"\n",
    "    Utilizes Spacy to extract nouns and verbs from ngrams\n",
    "    and build a new column with only these terms.\n",
    "    \"\"\"\n",
    "    nounAndVerbDict = {}\n",
    "    for row in range(len(df)):\n",
    "        text = \". \".join(df.iloc[row][column])\n",
    "        doc = nlp(text)\n",
    "        nounList = [chunk.text for chunk in doc.noun_chunks]\n",
    "        verbList = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "        nounAndVerbDict[df.index[row]] = nounList + verbList\n",
    "    dfNounAndVerb = pd.Series(nounAndVerbDict, name=newColumnName)\n",
    "    df[newColumnName] = dfNounAndVerb\n",
    "\n",
    "def partOfSpeechTagging(texts, POS=[\"NOUN\", \"ADJ\", \"ADV\", \"VERB\"]):\n",
    "    \"\"\"\n",
    "    https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#16buildingldamalletmodel\n",
    "    \"\"\"\n",
    "    textTagged = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        text_ = [token.lemma_ for token in doc if token.pos_ in POS]\n",
    "        textTagged.append(\" \".join(text_))\n",
    "    return textTagged\n",
    "\n",
    "def removeStopWords(texts):\n",
    "    \"\"\"\n",
    "    Takes in texts (words in a single string separated by spaces).\n",
    "    https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "    \"\"\"\n",
    "    textParsed = []\n",
    "    from nltk.corpus import stopwords\n",
    "    _stopWords = stopwords.words(\"english\")\n",
    "    _stopWords.extend([\"-pron-\", \"pron\"]) # these words appears in many ngrams without apparent meaning\n",
    "    textParsed = texts.apply(lambda x: \" \".join([word for word in x.split() if word not in (list(_stopWords))]))\n",
    "    return textParsed\n",
    "\n",
    "def getUniqueWordsColumn(df, column, newColumnName, nonWords=[]):\n",
    "    \"\"\"\n",
    "    Given a dataframe and column, constructs a new column with name newColumnName\n",
    "    of the unique words in  df[column].\n",
    "    The object in  df[column]  must be a list of strings.\n",
    "    Returns the updated dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ### deprecated ###\n",
    "\n",
    "    df[newColumnName] = df[column]\n",
    "    for row in range(len(df[newColumnName])):\n",
    "        df[newColumnName][row] = df[column].iloc[row]\n",
    "        string_list = []\n",
    "        for string in df[newColumnName].iloc[row]:\n",
    "            string_list.append(string.split(\" \")) # splits words into list of individual word strings\n",
    "        string_list = list(itertools.chain(*string_list)) # concatenates nested list into 1D list\n",
    "        string_list = list(set(string_list)) # grabs only unique string items\n",
    "        for nonword in nonWords:\n",
    "            if nonword in string_list:\n",
    "                string_list.remove(nonword)\n",
    "        df.loc[newColumnName].iloc[row] = string_list\n",
    "    return df\n",
    "\n",
    "def buildAdjacencyMatrixByColumn(df, column):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a column, constructs an adjacency matrix\n",
    "    of size [n x n] where  n  is the number of rows of the dataframe.\n",
    "    The adjacency matrix edge weights represent the number of similar elements.\n",
    "    The datatype in  df[column]  must be a list.\n",
    "    \"\"\"\n",
    "    n = len(df[column])\n",
    "    adjMatrix = np.zeros((n, n))\n",
    "    for n1 in range(n):\n",
    "        ngram1 = df[column].iloc[n1]\n",
    "        for n2 in range(n):\n",
    "            ngram2 = df[column].iloc[n2]\n",
    "            numSimilar = numSimilarStrings(ngram1, ngram2)\n",
    "            if n1 != n2 & numSimilar > 0: # removes recursive edges\n",
    "                adjMatrix[n1][n2] = numSimilar \n",
    "    return adjMatrix\n",
    "\n",
    "def buildAdjacencyListByColumn(df, column):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a column, constructs an adjacency list\n",
    "    as a nestd dictionary with  n  keys in the outermost dict, where\n",
    "    n  is the number of rows in the dataframe.\n",
    "    The adjacency list edge weights represent the number of similar elements.\n",
    "    The datatype in  df[column]  must be a list.\n",
    "    \"\"\"\n",
    "    n = len(df[column])\n",
    "\n",
    "    adjDict = {} # consider using the hash to represent nodes instead of numbers\n",
    "    for n1 in range(n):\n",
    "\n",
    "        nodeDict = {}\n",
    "        ngram1 = df[column].iloc[n1]\n",
    "        for n2 in range(n):\n",
    "            ngram2 = df[column].iloc[n2]\n",
    "            numSimilar = numSimilarStrings(ngram1, ngram2)\n",
    "            if n1 != n2 & numSimilar > 0: # removes recursive edges\n",
    "                nodeDict[n2] = {\"weight\" : numSimilar} # https://networkx.org/documentation/stable/reference/generated/networkx.convert.from_dict_of_dicts.html\n",
    "        adjDict[n1] = nodeDict\n",
    "\n",
    "    return adjDict\n",
    "\n",
    "def buildAdjacencyMatrixByCoOccurence(dfBOW):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X = np.array(dfBOW.values)\n",
    "    coocc = np.dot(X.T, X)\n",
    "    np.fill_diagonal(coocc, 0)\n",
    "    return coocc\n",
    "\n",
    "def buildDocVectorMatrixByColumn(df, docColumn, vectorSize=50, minCount=2, iterations=100, maxLen=30):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a column storing documents,\n",
    "    trains a gensim doc2vec model and outputs\n",
    "    the vector matrix.\n",
    "    \"\"\"\n",
    "    def read_corpus(documents):\n",
    "        for i, plot in enumerate(documents):\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(plot, max_len=maxLen), [i])\n",
    "\n",
    "    train_corpus = list(read_corpus(df[docColumn]))\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=vectorSize, min_count=minCount, epochs=iterations) # hyperparameters\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    docVectors = model.docvecs.vectors_docs\n",
    "    return docVectors\n",
    "\n",
    "def buildAdjacencyMatrixByCosineSimilarity(vecMatrix):\n",
    "    \"\"\"\n",
    "    Given a vector matrix, computes the cosine similarity score\n",
    "    between rows of the matrix and stores the values in an\n",
    "    adjacency matrix.\n",
    "    \"\"\"\n",
    "    n = len(vecMatrix)\n",
    "    cosSimMatrix = np.zeros((n,n))\n",
    "    for row in range(n):\n",
    "        a = vecMatrix[row]\n",
    "        for otherRow in range(n):\n",
    "            b = vecMatrix[otherRow]\n",
    "            cosSimMatrix[row][otherRow] = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    np.fill_diagonal(cosSimMatrix, 0)\n",
    "    return cosSimMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def loadTextFileIntoDataframe(filepath, columns, splittingChar=\"\\t\"):\n",
    "    \"\"\"\n",
    "    Opens the given filepath into a pandas dataframe.\n",
    "    Splits the list by the denoted character, by default tab.\n",
    "    Returns a pandas dataframe.\n",
    "    \"\"\"\n",
    "    with open(filepath) as file:\n",
    "        data = file.readlines()\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for line in data:\n",
    "        lineSplit = [line.split(splittingChar)]\n",
    "        if len(lineSplit[0]) < len(columns):\n",
    "            for x in range(len(columns) - len(lineSplit[0])):\n",
    "                lineSplit[0].append(\"0\")\n",
    "        lineDF = pd.DataFrame(lineSplit, columns=columns)\n",
    "        df = pd.concat([df, lineDF], axis=0, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def numSimilarStrings(stringList1, stringList2):\n",
    "    \"\"\"\n",
    "    Given two lists of strings, returns the number of strings they both share.\n",
    "    In other words, the size of the subset intersection of stringList1 and stringList2.\n",
    "    \"\"\"\n",
    "\n",
    "    # is there a faster way to do this with sets?\n",
    "\n",
    "    count = 0\n",
    "    for string in stringList1:\n",
    "        if string in stringList2:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "def subtractListsOfInts(_list1, _list2):\n",
    "    \"\"\"\n",
    "    Given two lists of items, returns a list of items\n",
    "    in _list1 and not in _list2.\n",
    "    Utilizes collections.Counter\n",
    "    Returns a list of items.\n",
    "\n",
    "    https://stackoverflow.com/questions/2070643/subtracting-two-lists-in-python\n",
    "    \"\"\"\n",
    "    _set1 = Counter(_list1)\n",
    "    _set2 = Counter(_list2)\n",
    "    _set1_2 = _set1 - _set2\n",
    "    return list(_set1_2.elements())\n",
    "\n",
    "def cosineSimilarityNumMostSimilar(model, word, target_list, num):\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n",
    "    \"\"\"\n",
    "    cosine_dict ={}\n",
    "    word_list = []\n",
    "    a = model[word]\n",
    "    for item in target_list :\n",
    "        if item != word :\n",
    "            b = model [item]\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            cosine_dict[item] = cos_sim\n",
    "    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in descending order \n",
    "    for item in dist_sort:\n",
    "        word_list.append((item[0],  item[1]))\n",
    "    return word_list[0:num]\n",
    "\n",
    "def standardizeDataColumn(df, column, newColumnName):\n",
    "    \"\"\"\n",
    "    Standardizes the column of the dataframe df.\n",
    "    Adds the new column newColumnName to the dataframe inplace.\n",
    "    Utilizes SKLearn.preprocessing.standardscaler.\n",
    "    Mean is 0, variance is 1\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaledSeries = pd.Series(np.reshape(scaler.fit_transform(np.array(df[column]).reshape(-1, 1)), (-1)), name=newColumnName, index=df.index)\n",
    "    scaledSeries = scaledSeries + 1 # adding 1 scales most (~98%) to be in range [0, 2] for graphing purposes\n",
    "    df[newColumnName] = scaledSeries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization -------------------------------------------------------------------------------------------\n",
    "\n",
    "def visualizeNetworkHTML(_graph, _filename, _width=\"1920px\", _height=\"1080px\", _physics=False):\n",
    "    \"\"\"\n",
    "    Given a NetworkX graph and the filename to save to, builds an HTML\n",
    "    graph of that network. Optional parameters are width and height of graph.\n",
    "    Uses pyvis to build an interactive HTML graph of a NetworkX graph.\n",
    "    Uses NetworkX for graph storage.\n",
    "    \"\"\"\n",
    "    _net = Network(width=_width, height=_height, notebook=True)\n",
    "    _net.toggle_physics(_physics)\n",
    "    _net.barnes_hut()\n",
    "    _net.from_nx(_graph)\n",
    "    _net.show(_filename)\n",
    "\n",
    "def plot_top_words_one_topic(model, feature_names, n_top_words, title):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "    \"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        plt.barh(top_features, weights, height=0.7)\n",
    "        plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "        plt.suptitle(title, fontsize=25)\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title, n_topics):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, n_topics, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=25)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def drawNetworkgraph(networkx_graph,notebook=True,output_filename='graph.html',show_buttons=True,only_physics_buttons=False,\n",
    "                height=None,width=None,bgcolor=None,font_color=None,pyvis_options=None):\n",
    "    \"\"\"\n",
    "    https://gist.github.com/maciejkos/e3bc958aac9e7a245dddff8d86058e17\n",
    "    \n",
    "    This function accepts a networkx graph object,\n",
    "    converts it to a pyvis network object preserving its node and edge attributes,\n",
    "    and both returns and saves a dynamic network visualization.\n",
    "    Valid node attributes include:\n",
    "        \"size\", \"value\", \"title\", \"x\", \"y\", \"label\", \"color\".\n",
    "        (For more info: https://pyvis.readthedocs.io/en/latest/documentation.html#pyvis.network.Network.add_node)\n",
    "    Valid edge attributes include:\n",
    "        \"arrowStrikethrough\", \"hidden\", \"physics\", \"title\", \"value\", \"width\"\n",
    "        (For more info: https://pyvis.readthedocs.io/en/latest/documentation.html#pyvis.network.Network.add_edge)\n",
    "    Args:\n",
    "        networkx_graph: The graph to convert and display\n",
    "        notebook: Display in Jupyter?\n",
    "        output_filename: Where to save the converted network\n",
    "        show_buttons: Show buttons in saved version of network?\n",
    "        only_physics_buttons: Show only buttons controlling physics of network?\n",
    "        height: height in px or %, e.g, \"750px\" or \"100%\n",
    "        width: width in px or %, e.g, \"750px\" or \"100%\n",
    "        bgcolor: background color, e.g., \"black\" or \"#222222\"\n",
    "        font_color: font color,  e.g., \"black\" or \"#222222\"\n",
    "        pyvis_options: provide pyvis-specific options (https://pyvis.readthedocs.io/en/latest/documentation.html#pyvis.options.Options.set)\n",
    "    \"\"\"\n",
    "\n",
    "    # make a pyvis network\n",
    "    network_class_parameters = {\"notebook\": notebook, \"height\": height, \"width\": width, \"bgcolor\": bgcolor, \"font_color\": font_color}\n",
    "    pyvis_graph = net.Network(**{parameter_name: parameter_value for parameter_name, parameter_value in network_class_parameters.items() if parameter_value})\n",
    "\n",
    "    # for each node and its attributes in the networkx graph\n",
    "    for node,node_attrs in networkx_graph.nodes(data=True):\n",
    "        pyvis_graph.add_node(node,**node_attrs)\n",
    "\n",
    "    # for each edge and its attributes in the networkx graph\n",
    "    for source,target,edge_attrs in networkx_graph.edges(data=True):\n",
    "        # if value/width not specified directly, and weight is specified, set 'value' to 'weight'\n",
    "        if not 'value' in edge_attrs and not 'width' in edge_attrs and 'weight' in edge_attrs:\n",
    "            # place at key 'value' the weight of the edge\n",
    "            edge_attrs['value']=edge_attrs['weight']\n",
    "        # add the edge\n",
    "        pyvis_graph.add_edge(source,target,**edge_attrs)\n",
    "\n",
    "    # turn buttons on\n",
    "    if show_buttons:\n",
    "        if only_physics_buttons:\n",
    "            pyvis_graph.show_buttons(filter_=['physics'])\n",
    "        else:\n",
    "            pyvis_graph.show_buttons()\n",
    "\n",
    "    # pyvis-specific options\n",
    "    if pyvis_options:\n",
    "        pyvis_graph.set_options(pyvis_options)\n",
    "\n",
    "    # return and also save\n",
    "    return pyvis_graph.show(output_filename)\n",
    "\n",
    "def display_closestwords_tsnescatterplot(model, word, size):\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n",
    "    \"\"\"\n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    close_words = model.similar_by_word(word)\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "            \n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "        plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "        plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Dataframe and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard coded things\n",
    "columnListNGrams = [\"hash\", \"ngram\", \"ngram_lc\", \"ngram_tokens\", \"ngram_count\", \"term_freq\", \"doc_count\", \"date_added\"]\n",
    "columnListKeywords = [\"hash\", \"keywords\", \"keywords_lc\", \"keyword_tokens\", \"keyword_score\", \"doc_count\", \"insert_date\"]\n",
    "\n",
    "non_words = [\"a\", \"at\", \"an\", \"am\", \"and\", \"that\", \"like\", \"for\", \"by\", \"i\", \"in\", \"of\", \"or\", \"be\", \"use\", \"as\", \"on\", \"the\", \"to\", \"with\", \"-pron-\"]\n",
    "\n",
    "filenameAnteNGrams = \"data/doc_ngrams/sample.fgrep.antediluvian.txt\"\n",
    "filepathHennigNGrams = \"data/doc_ngrams/sample.fgrep.Hennig86.txt\"\n",
    "filenameAnteKeywords = \"data/doc_keywords/sample.fgrep.antediluvian.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# load test files into dataframe\\n\\nindexCol = columnListNGrams[0]\\ndf_antedivulian = loadTextFileIntoDataframe(filepath=filenameAnteNGrams, columns=columnListNGrams)\\ndf_hennig = loadTextFileIntoDataframe(filepath=filepathHennigNGrams, columns=columnListNGrams)\\ndf_antedivulian[\"topic\"] = \"antediluvian\"\\ndf_hennig[\"topic\"] = \"hennig86\"\\ndf = pd.concat([df_antedivulian, df_hennig])\\n\\n# process data columns\\ndf[\"ngram_lc_tagged\"] = partOfSpeechTagging(df[\"ngram_lc\"])\\ndf[\"ngram_lc_tagged\"] = removeStopWords(df[\"ngram_lc_tagged\"])\\ndf = df.groupby(indexCol).agg(list)\\n\\nyakeScoreCol = columnListNGrams[5]\\nfor n in range(len(df)):\\n    term_freq_list = df[yakeScoreCol].iloc[n]\\n    df[yakeScoreCol].iloc[n] = term_freq_list[0]\\n    df[\"topic\"].iloc[n] = df[\"topic\"].iloc[n][0]\\n    if (len(df[\"ngram_lc_tagged\"].iloc[n])) == 0: # if there are no noun/verb phrases from spacy preprocessing\\n        df[\"ngram_lc_tagged\"][n] = df[\"ngram_lc\"].iloc[n] # replace it with the lowercase ngram(s)\\ndf[yakeScoreCol] = df[yakeScoreCol].astype(float)\\n\\nstandardizeDataColumn(df, yakeScoreCol, \"normalized_term_freq\")\\ngetDocColumn(df, \"ngram_lc_tagged\", \"ngram_lc_tagged_doc\")\\ngetDocColumn(df, \"ngram_lc\", \"ngram_lc_doc\")\\ndf[\"topic_num\"] = (df[\"topic\"] == \"antediluvian\").astype(int)\\n\\n# save the dataframe for later use\\ndf.to_csv(\"test_data_processed.csv\")\\n\\ndf.head()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# load test files into dataframe\n",
    "\n",
    "indexCol = columnListNGrams[0]\n",
    "df_antedivulian = loadTextFileIntoDataframe(filepath=filenameAnteNGrams, columns=columnListNGrams)\n",
    "df_hennig = loadTextFileIntoDataframe(filepath=filepathHennigNGrams, columns=columnListNGrams)\n",
    "df_antedivulian[\"topic\"] = \"antediluvian\"\n",
    "df_hennig[\"topic\"] = \"hennig86\"\n",
    "df = pd.concat([df_antedivulian, df_hennig])\n",
    "\n",
    "# process data columns\n",
    "df[\"ngram_lc_tagged\"] = partOfSpeechTagging(df[\"ngram_lc\"])\n",
    "df[\"ngram_lc_tagged\"] = removeStopWords(df[\"ngram_lc_tagged\"])\n",
    "df = df.groupby(indexCol).agg(list)\n",
    "\n",
    "yakeScoreCol = columnListNGrams[5]\n",
    "for n in range(len(df)):\n",
    "    term_freq_list = df[yakeScoreCol].iloc[n]\n",
    "    df[yakeScoreCol].iloc[n] = term_freq_list[0]\n",
    "    df[\"topic\"].iloc[n] = df[\"topic\"].iloc[n][0]\n",
    "    if (len(df[\"ngram_lc_tagged\"].iloc[n])) == 0: # if there are no noun/verb phrases from spacy preprocessing\n",
    "        df[\"ngram_lc_tagged\"][n] = df[\"ngram_lc\"].iloc[n] # replace it with the lowercase ngram(s)\n",
    "df[yakeScoreCol] = df[yakeScoreCol].astype(float)\n",
    "\n",
    "standardizeDataColumn(df, yakeScoreCol, \"normalized_term_freq\")\n",
    "getDocColumn(df, \"ngram_lc_tagged\", \"ngram_lc_tagged_doc\")\n",
    "getDocColumn(df, \"ngram_lc\", \"ngram_lc_doc\")\n",
    "df[\"topic_num\"] = (df[\"topic\"] == \"antediluvian\").astype(int)\n",
    "\n",
    "# save the dataframe for later use\n",
    "df.to_csv(\"test_data_processed.csv\")\n",
    "\n",
    "df.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>ngram_lc</th>\n",
       "      <th>ngram_tokens</th>\n",
       "      <th>ngram_count</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_count</th>\n",
       "      <th>date_added</th>\n",
       "      <th>topic</th>\n",
       "      <th>ngram_lc_tagged</th>\n",
       "      <th>standardized_term_freq</th>\n",
       "      <th>ngram_lc_tagged_doc</th>\n",
       "      <th>ngram_lc_doc</th>\n",
       "      <th>topic_num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hash</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3002e8a37ec9d00a67bdf0004b8628c35d72068d</th>\n",
       "      <td>['antediluvian', 'antediluvian humanity']</td>\n",
       "      <td>['antediluvian', 'antediluvian humanity']</td>\n",
       "      <td>['1', '2']</td>\n",
       "      <td>['1', '1']</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>['1', '1']</td>\n",
       "      <td>['\\\\N\\n', '\\\\N\\n']</td>\n",
       "      <td>antediluvian</td>\n",
       "      <td>['antediluvian', 'antediluvian humanity']</td>\n",
       "      <td>-0.654965</td>\n",
       "      <td>antediluvian. antediluvian humanity.</td>\n",
       "      <td>antediluvian. antediluvian humanity.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005b3bf055ddcb3c25e4742a72ee16728934efd</th>\n",
       "      <td>['antediluvian', 'antediluvian refrain', 'foll...</td>\n",
       "      <td>['antediluvian', 'antediluvian refrain', 'foll...</td>\n",
       "      <td>['1', '2', '4', '5']</td>\n",
       "      <td>['1', '1', '1', '1']</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>['1', '1', '1', '1']</td>\n",
       "      <td>['\\\\N\\n', '\\\\N\\n', '\\\\N\\n', '\\\\N\\n']</td>\n",
       "      <td>antediluvian</td>\n",
       "      <td>['antediluvian', 'antediluvian refrain', 'foll...</td>\n",
       "      <td>-0.004052</td>\n",
       "      <td>antediluvian. antediluvian refrain. follow ant...</td>\n",
       "      <td>antediluvian. antediluvian refrain. follow by ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005ebfe5508340797dbfcce8454f3d3f6f76eb1</th>\n",
       "      <td>['antediluvian', 'antediluvian dream', 'cave o...</td>\n",
       "      <td>['antediluvian', 'antediluvian dream', 'cave o...</td>\n",
       "      <td>['1', '2', '4', '5', '5']</td>\n",
       "      <td>['1', '1', '1', '1', '1']</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>['1', '1', '1', '1', '1']</td>\n",
       "      <td>['2021-09-03\\n', '2021-09-03\\n', '2021-09-03\\n...</td>\n",
       "      <td>antediluvian</td>\n",
       "      <td>['antediluvian', 'antediluvian dream', 'cave a...</td>\n",
       "      <td>-0.459385</td>\n",
       "      <td>antediluvian. antediluvian dream. cave antedil...</td>\n",
       "      <td>antediluvian. antediluvian dream. cave of -pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30064ae161de1e9a96992be108c195796f13e72a</th>\n",
       "      <td>['Hennig86 program', 'routine in the Hennig86'...</td>\n",
       "      <td>['hennig86 program', 'routine in the hennig86'...</td>\n",
       "      <td>['2', '4', '5', '1']</td>\n",
       "      <td>['1', '1', '1', '1']</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>['1', '1', '1', '1']</td>\n",
       "      <td>['\\\\N\\n', '\\\\N\\n', '\\\\N\\n', '\\\\N\\n']</td>\n",
       "      <td>hennig86</td>\n",
       "      <td>['hennig86 program', 'routine hennig86', 'rout...</td>\n",
       "      <td>-0.202622</td>\n",
       "      <td>hennig86 program. routine hennig86. routine he...</td>\n",
       "      <td>hennig86 program. routine in the hennig86. rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30136ab3788ab8e8be6b939901ec669a41ef896a</th>\n",
       "      <td>['antediluvian']</td>\n",
       "      <td>['antediluvian']</td>\n",
       "      <td>['1']</td>\n",
       "      <td>['1']</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>['1']</td>\n",
       "      <td>['\\\\N\\n']</td>\n",
       "      <td>antediluvian</td>\n",
       "      <td>['antediluvian']</td>\n",
       "      <td>-0.556359</td>\n",
       "      <td>antediluvian.</td>\n",
       "      <td>antediluvian.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      ngram  \\\n",
       "hash                                                                                          \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d          ['antediluvian', 'antediluvian humanity']   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd  ['antediluvian', 'antediluvian refrain', 'foll...   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  ['antediluvian', 'antediluvian dream', 'cave o...   \n",
       "30064ae161de1e9a96992be108c195796f13e72a  ['Hennig86 program', 'routine in the Hennig86'...   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                                   ['antediluvian']   \n",
       "\n",
       "                                                                                   ngram_lc  \\\n",
       "hash                                                                                          \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d          ['antediluvian', 'antediluvian humanity']   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd  ['antediluvian', 'antediluvian refrain', 'foll...   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  ['antediluvian', 'antediluvian dream', 'cave o...   \n",
       "30064ae161de1e9a96992be108c195796f13e72a  ['hennig86 program', 'routine in the hennig86'...   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                                   ['antediluvian']   \n",
       "\n",
       "                                                       ngram_tokens  \\\n",
       "hash                                                                  \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d                 ['1', '2']   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd       ['1', '2', '4', '5']   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  ['1', '2', '4', '5', '5']   \n",
       "30064ae161de1e9a96992be108c195796f13e72a       ['2', '4', '5', '1']   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                      ['1']   \n",
       "\n",
       "                                                        ngram_count  \\\n",
       "hash                                                                  \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d                 ['1', '1']   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd       ['1', '1', '1', '1']   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  ['1', '1', '1', '1', '1']   \n",
       "30064ae161de1e9a96992be108c195796f13e72a       ['1', '1', '1', '1']   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                      ['1']   \n",
       "\n",
       "                                          term_freq  \\\n",
       "hash                                                  \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d   0.000010   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd   0.000281   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1   0.000091   \n",
       "30064ae161de1e9a96992be108c195796f13e72a   0.000198   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a   0.000051   \n",
       "\n",
       "                                                          doc_count  \\\n",
       "hash                                                                  \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d                 ['1', '1']   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd       ['1', '1', '1', '1']   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  ['1', '1', '1', '1', '1']   \n",
       "30064ae161de1e9a96992be108c195796f13e72a       ['1', '1', '1', '1']   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                      ['1']   \n",
       "\n",
       "                                                                                 date_added  \\\n",
       "hash                                                                                          \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d                                 ['\\\\N\\n', '\\\\N\\n']   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd               ['\\\\N\\n', '\\\\N\\n', '\\\\N\\n', '\\\\N\\n']   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  ['2021-09-03\\n', '2021-09-03\\n', '2021-09-03\\n...   \n",
       "30064ae161de1e9a96992be108c195796f13e72a               ['\\\\N\\n', '\\\\N\\n', '\\\\N\\n', '\\\\N\\n']   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                                          ['\\\\N\\n']   \n",
       "\n",
       "                                                 topic  \\\n",
       "hash                                                     \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d  antediluvian   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd  antediluvian   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  antediluvian   \n",
       "30064ae161de1e9a96992be108c195796f13e72a      hennig86   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a  antediluvian   \n",
       "\n",
       "                                                                            ngram_lc_tagged  \\\n",
       "hash                                                                                          \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d          ['antediluvian', 'antediluvian humanity']   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd  ['antediluvian', 'antediluvian refrain', 'foll...   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  ['antediluvian', 'antediluvian dream', 'cave a...   \n",
       "30064ae161de1e9a96992be108c195796f13e72a  ['hennig86 program', 'routine hennig86', 'rout...   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                                   ['antediluvian']   \n",
       "\n",
       "                                          standardized_term_freq  \\\n",
       "hash                                                               \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d               -0.654965   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd               -0.004052   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1               -0.459385   \n",
       "30064ae161de1e9a96992be108c195796f13e72a               -0.202622   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a               -0.556359   \n",
       "\n",
       "                                                                        ngram_lc_tagged_doc  \\\n",
       "hash                                                                                          \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d               antediluvian. antediluvian humanity.   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd  antediluvian. antediluvian refrain. follow ant...   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  antediluvian. antediluvian dream. cave antedil...   \n",
       "30064ae161de1e9a96992be108c195796f13e72a  hennig86 program. routine hennig86. routine he...   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                                      antediluvian.   \n",
       "\n",
       "                                                                               ngram_lc_doc  \\\n",
       "hash                                                                                          \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d               antediluvian. antediluvian humanity.   \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd  antediluvian. antediluvian refrain. follow by ...   \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1  antediluvian. antediluvian dream. cave of -pro...   \n",
       "30064ae161de1e9a96992be108c195796f13e72a  hennig86 program. routine in the hennig86. rou...   \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a                                      antediluvian.   \n",
       "\n",
       "                                          topic_num  \n",
       "hash                                                 \n",
       "3002e8a37ec9d00a67bdf0004b8628c35d72068d          1  \n",
       "3005b3bf055ddcb3c25e4742a72ee16728934efd          1  \n",
       "3005ebfe5508340797dbfcce8454f3d3f6f76eb1          1  \n",
       "30064ae161de1e9a96992be108c195796f13e72a          0  \n",
       "30136ab3788ab8e8be6b939901ec669a41ef896a          1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the processed data CSV\n",
    "df = pd.read_csv(\"test_data_processed.csv\")\n",
    "df.set_index(\"hash\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of all words that appear in the dataset\n",
    "\n",
    "from itertools import chain\n",
    "masterTermList = []\n",
    "for eachList in df[\"ngram_lc\"].values:\n",
    "    splitList = []\n",
    "    for string in eachList:\n",
    "        splitList.append(string.split(\" \"))\n",
    "    masterTermList.append(splitList)\n",
    "firstNest = list(chain(*masterTermList))\n",
    "secondNest = list(chain(*firstNest))\n",
    "masterTerms = list(dict.fromkeys(secondNest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of times each word appears in an ngram\n",
    "dfBoW = getBagOfWordsDF(df, \"ngram_lc_doc\")\n",
    "dfBoWTag = getBagOfWordsDF(df, \"ngram_lc_tagged_doc\")\n",
    "wordCountDict = {}\n",
    "wordCountDictTag = {}\n",
    "for column in dfBoW.columns:\n",
    "    wordCountDict[column] = np.sum(dfBoW[column])\n",
    "for column in dfBoWTag.columns:\n",
    "    wordCountDictTag[column] = np.sum(dfBoWTag[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Doc2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/s8vtyz9x38v4szdk13kx7g6w0000gn/T/ipykernel_62389/1834578130.py:152: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  docVectors = model.docvecs.vectors_docs\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'vectors_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/ian/Documents/GitHub/General-Index-Visualization/data pipeline.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39m# build Doc2Vec embedding\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000013?line=1'>2</a>\u001b[0m docVectors \u001b[39m=\u001b[39m buildDocVectorMatrixByColumn(df, \u001b[39m\"\u001b[39;49m\u001b[39mngram_lc_doc\u001b[39;49m\u001b[39m\"\u001b[39;49m, vectorSize\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, minCount\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m# optimize the hyperparameters of this\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000013?line=2'>3</a>\u001b[0m docVecDF \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(docVectors)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000013?line=3'>4</a>\u001b[0m docVecDF\u001b[39m.\u001b[39mdescribe()\n",
      "\u001b[1;32m/Users/ian/Documents/GitHub/General-Index-Visualization/data pipeline.ipynb Cell 16\u001b[0m in \u001b[0;36mbuildDocVectorMatrixByColumn\u001b[0;34m(df, docColumn, vectorSize, minCount, iterations, maxLen)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000013?line=149'>150</a>\u001b[0m model\u001b[39m.\u001b[39mbuild_vocab(train_corpus)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000013?line=150'>151</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(train_corpus, total_examples\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mcorpus_count, epochs\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mepochs)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000013?line=151'>152</a>\u001b[0m docVectors \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdocvecs\u001b[39m.\u001b[39;49mvectors_docs\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000013?line=152'>153</a>\u001b[0m \u001b[39mreturn\u001b[39;00m docVectors\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'vectors_docs'"
     ]
    }
   ],
   "source": [
    "# build Doc2Vec embedding\n",
    "docVectors = buildDocVectorMatrixByColumn(df, \"ngram_lc_doc\", vectorSize=20, minCount=1) # optimize the hyperparameters of this\n",
    "docVecDF = pd.DataFrame(docVectors)\n",
    "docVecDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec cosine similarity heatmap\n",
    "cosSimMatrixDocs = buildAdjacencyMatrixByCosineSimilarity(docVectors)\n",
    "figure(figsize=(18,9), dpi=300)\n",
    "sns.heatmap(cosSimMatrixDocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE Dimensional Reduction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE Projection of n-dimensional document vectors to 2 dimensions\n",
    "from sklearn.manifold import TSNE\n",
    "projection = TSNE(init='pca').fit_transform(docVectors)\n",
    "figure(figsize=(6,6), dpi=200)\n",
    "plt.scatter(*projection.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Manifold Approximation and Projection (UMAP) Dimensional Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "import umap\n",
    "\n",
    "UMAP = umap.UMAP()\n",
    "reducedEmbedding = UMAP.fit_transform(docVectors)\n",
    "figure(figsize=(18,9), dpi=300)\n",
    "plt.scatter(\n",
    "    reducedEmbedding[:, 0],\n",
    "    reducedEmbedding[:, 1],\n",
    "    # c=[sns.color_palette()[x] for x in penguins.species_short.map({\"Adelie\":0, \"Chinstrap\":1, \"Gentoo\":2})])\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the Doc2Vec Embedding', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN, a single slice of HDBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "db = DBSCAN(eps=0.20, min_samples=2).fit(reducedEmbedding) # optimize hyperparameters, or just use HDBSCAN\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(labels) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(docVectors, labels))\n",
    "\n",
    "figure(figsize=(18,9), dpi=300)\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = labels\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "        continue; # removes noise from plot\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = docVectors[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = docVectors[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN\n",
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True) # optimize hyperparameters\n",
    "clusterer.fit(reducedEmbedding)\n",
    "hdbscanLabels = clusterer.labels_\n",
    "hdbscanProbabilities = clusterer.probabilities_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(hdbscanLabels) - (1 if -1 in hdbscanLabels else 0)\n",
    "n_noise_ = list(hdbscanLabels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(docVectors, hdbscanLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(18,9), dpi=300)\n",
    "clusterer.minimum_spanning_tree_.plot(edge_cmap='viridis',\n",
    "                                      edge_alpha=0.6,\n",
    "                                      node_size=80,\n",
    "                                      edge_linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(18,9), dpi=300)\n",
    "clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cTF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Networks and Partitioning Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Networkx Graphs...\")\n",
    "t0 = time()\n",
    "Gcossim = nx.from_numpy_array(cosSimMatrixDocs)\n",
    "print(\"Done in %0.3fs.\\n\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding Node Metadata...\")\n",
    "t0 = time()\n",
    "numCommunities = 0\n",
    "numNodes = len(df)\n",
    "nodes = list(range(numNodes))\n",
    "scalingFactor = 5. # scales the node size for visibility\n",
    "communitySets = communitySetsCossineSim\n",
    "G = Gcossim\n",
    "adjMatrix = cosSimMatrixDocs\n",
    "\n",
    "node_to_community = {}\n",
    "\n",
    "for set in communitySets:\n",
    "    for x in set:\n",
    "        node_to_community[x] = numCommunities\n",
    "    numCommunities = numCommunities + 1\n",
    "\n",
    "for n in nodes:\n",
    "    # 'title': hash (i.e. dataframe index)\n",
    "    # 'group': partition\n",
    "    # 'value': topic (empty for now)\n",
    "    # 'size': normalized YAKE score\n",
    "    G.nodes[n][\"title\"] = df.index[n] # this (clearly) doesn't work for matrices based on words (e.g. cooccurence)\n",
    "    G.nodes[n][\"group\"] = node_to_community[n]\n",
    "    G.nodes[n][\"size\"] = df[\"normalized_term_freq\"][n] * scalingFactor\n",
    "print(\"Done in %0.3fs.\\n\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network/Topic Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the network you would like to visualize below as a NetworkX object in the first parameter\n",
    "# recommended to only run this after validating the network with the above graphs and statistics\n",
    "# if it is taking a while to run/load the image, set _physics=False\n",
    "visualizeNetworkHTML(Gcossim, _filename=\"test vector cosine.html\", _width=\"3840px\", _height=\"2160px\", _physics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ian/Documents/GitHub/General-Index-Visualization/data pipeline.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000033?line=0'>1</a>\u001b[0m \u001b[39m# bridge node\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ian/Documents/GitHub/General-Index-Visualization/data%20pipeline.ipynb#ch0000033?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mloc[\u001b[39m\"\u001b[39m\u001b[39m3bad1c0f3b983ab81ddff3f5e90e687310269d16\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# bridge node\n",
    "df.loc[\"3bad1c0f3b983ab81ddff3f5e90e687310269d16\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
